{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googlecolab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Could not find a version that satisfies the requirement googlecolab (from versions: )\n",
      "No matching distribution found for googlecolab\n",
      "You are using pip version 19.0.3, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install googlecolab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/14/e4538c2bc3ae9f4ce6f6ce7ef1180da05abc4a617afba798268232b01d0d/tensorflow-1.13.1-cp37-cp37m-win_amd64.whl (63.1MB)\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\jeshi\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in c:\\users\\jeshi\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.7)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in c:\\users\\jeshi\\anaconda3\\lib\\site-packages (from tensorflow) (1.0.9)\n",
      "Collecting protobuf>=3.6.1 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/34/ef/f020691889031a8e1d8cb20711daa43cfe999e0768ff6903c4bf70c2eecd/protobuf-3.7.1-cp37-cp37m-win_amd64.whl (986kB)\n",
      "Collecting tensorboard<1.14.0,>=1.13.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
      "Collecting absl-py>=0.1.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/da/3f/9b0355080b81b15ba6a9ffcf1f5ea39e307a2778b2f2dc8694724e8abd5b/absl-py-0.7.1.tar.gz (99kB)\n",
      "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\jeshi\\anaconda3\\lib\\site-packages (from tensorflow) (0.31.1)\n",
      "Collecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\jeshi\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.0)\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/2a/22/bd327063dd0bdf9d8d640b3185b760707842160e69df909db3fcaab5b758/grpcio-1.20.1-cp37-cp37m-win_amd64.whl (1.6MB)\n",
      "Requirement already satisfied: h5py in c:\\users\\jeshi\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jeshi\\anaconda3\\lib\\site-packages (from protobuf>=3.6.1->tensorflow) (40.2.0)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<1.14.0,>=1.13.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/f5/e4/d8c18f2555add57ff21bf25af36d827145896a07607486cc79a2aea641af/Markdown-3.1-py2.py3-none-any.whl (87kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\jeshi\\anaconda3\\lib\\site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow) (0.14.1)\n",
      "Collecting mock>=2.0.0 (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: absl-py, gast, termcolor\n",
      "  Building wheel for absl-py (setup.py): started\n",
      "  Building wheel for absl-py (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\jeshi\\AppData\\Local\\pip\\Cache\\wheels\\ee\\98\\38\\46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48\n",
      "  Building wheel for gast (setup.py): started\n",
      "  Building wheel for gast (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\jeshi\\AppData\\Local\\pip\\Cache\\wheels\\5c\\2e\\7e\\a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\jeshi\\AppData\\Local\\pip\\Cache\\wheels\\7c\\06\\54\\bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "Successfully built absl-py gast termcolor\n",
      "Installing collected packages: astor, protobuf, markdown, absl-py, grpcio, tensorboard, mock, tensorflow-estimator, gast, termcolor, tensorflow\n",
      "Successfully installed absl-py-0.7.1 astor-0.7.1 gast-0.2.2 grpcio-1.20.1 markdown-3.1 mock-3.0.5 protobuf-3.7.1 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0 termcolor-1.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 19.0.3, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\users\\jeshi\\anaconda3\\lib\\site-packages (from keras) (1.1.0)\n",
      "Collecting keras-preprocessing>=1.0.5 (from keras)\n",
      "  Downloading https://files.pythonhosted.org/packages/c0/bf/0315ef6a9fd3fc2346e85b0ff1f5f83ca17073f2c31ac719ab2e4da0d4a3/Keras_Preprocessing-1.0.9-py2.py3-none-any.whl (59kB)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\jeshi\\anaconda3\\lib\\site-packages (from keras) (3.13)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\jeshi\\anaconda3\\lib\\site-packages (from keras) (1.11.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\jeshi\\anaconda3\\lib\\site-packages (from keras) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\jeshi\\anaconda3\\lib\\site-packages (from keras) (1.15.1)\n",
      "Collecting keras-applications>=1.0.6 (from keras)\n",
      "  Downloading https://files.pythonhosted.org/packages/90/85/64c82949765cfb246bbdaf5aca2d55f400f792655927a017710a78445def/Keras_Applications-1.0.7-py2.py3-none-any.whl (51kB)\n",
      "Installing collected packages: keras-preprocessing, keras-applications, keras\n",
      "Successfully installed keras-2.2.4 keras-applications-1.0.7 keras-preprocessing-1.0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 19.0.3, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructor Turn One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we **preprocess** data when we build machine learning pipelines?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We preprocess data for two principle reasons:\n",
    "\n",
    "1. To transform the data to better suit a model's underlying assumptions.\n",
    "2. To format the data in the way a model expects.\n",
    "\n",
    "Today, we're concerned with this second reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs to Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the input to a neural network look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs to neural networks are **vectors**. Each entry in the vector corresponds to a feature, which the net uses to make predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crucially, these vectors contain can contain only _numerical_ data. They _cannot_ contain string data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good!\n",
    "good_input_row1 = [1.3, 2.2, 5.4, 5.8, 0]\n",
    "good_input_row2 = [1.3, 2.2, 5.4, 5.8, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad...\n",
    "bad_input_row1 = [1.3, 2.2, 5.4, 5.8, 'dog']\n",
    "bad_input_row2 = [1.3, 2.2, 5.4, 5.8, 'cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This poses a problem when we want to train a neural network on categorical data, such as the classic [Iris data set](https://archive.ics.uci.edu/ml/datasets/Iris\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/5/56/Kosaciec_szczecinkowaty_Iris_setosa.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read from: https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.names\n",
    "names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "df = pd.read_csv(\n",
    "    'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width           class\n",
       "145           6.7          3.0           5.2          2.3  Iris-virginica\n",
       "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
       "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
       "148           6.2          3.4           5.4          2.3  Iris-virginica\n",
       "149           5.9          3.0           5.1          1.8  Iris-virginica"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the entries `iris-virginica`\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all of our data is numerical..._Except_ for the data in that `class` column, which contains strings.\n",
    "\n",
    "The `class` column will contain one of three values:\n",
    "\n",
    "1. `iris-setosa`\n",
    "2. `iris-versicolour`\n",
    "3. `iris-virginica`\n",
    "\n",
    "As these are not numerical values, we can't use them to fit our nnet. To fix this, we must convert each class label to a numerical value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do this via the following steps:\n",
    "\n",
    "1. **Label Encoding**. First, we convert the three possible classes to integer labels. E.g., `iris-setosa` will be `1`; `iris-versicolour`, `2`; and `iris-virginica`, `3`.\n",
    "2. **One-Hot Encoding**. Then, we set each row's `class` value to an _array_. This array will have a `1` in whichever slot corresponds to the integer label. E.g., after one-hot encoding, a row with the class `iris-setosa` will have the array `[1, 0, 0]`. A row with class `iris-virginica`, the array `[0, 0, 1]`; etc.\n",
    "\n",
    "In many cases, categories in the data sets you work with will already be label-encoded. In this case, you can apply one-hot encoding immediately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.0 1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.0 3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.0 3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.0 1.4 0.1]\n",
      " [4.3 3.0 1.1 0.1]\n",
      " [5.8 4.0 1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.0 0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.0 3.0 1.6 0.2]\n",
      " [5.0 3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.0 3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [4.4 3.0 1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.0 3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.0 3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.0 1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.0 3.3 1.4 0.2]\n",
      " [7.0 3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.0 1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1.0]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.0 2.0 3.5 1.0]\n",
      " [5.9 3.0 4.2 1.5]\n",
      " [6.0 2.2 4.0 1.0]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.0 4.5 1.5]\n",
      " [5.8 2.7 4.1 1.0]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.0 1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.0 4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.0 5.0 1.7]\n",
      " [6.0 2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1.0]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1.0]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.0 2.7 5.1 1.6]\n",
      " [5.4 3.0 4.5 1.5]\n",
      " [6.0 3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.0 4.1 1.3]\n",
      " [5.5 2.5 4.0 1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.0 4.6 1.4]\n",
      " [5.8 2.6 4.0 1.2]\n",
      " [5.0 2.3 3.3 1.0]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.0 4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.0 1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.0 2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.0 5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.0 5.8 2.2]\n",
      " [7.6 3.0 6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2.0]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.0 5.5 2.1]\n",
      " [5.7 2.5 5.0 2.0]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.0 5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.0 2.2 5.0 1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2.0]\n",
      " [7.7 2.8 6.7 2.0]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.0 1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.0 4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.0 5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2.0]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.0 6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.0 3.0 4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.0 5.2 2.3]\n",
      " [6.3 2.5 5.0 1.9]\n",
      " [6.5 3.0 5.2 2.0]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.0 5.1 1.8]]\n",
      "y ['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica']\n"
     ]
    }
   ],
   "source": [
    "# Step 0: Reformat data\n",
    "data = df.values\n",
    "X = data[:, 0:4]\n",
    "y = data[:, 4]\n",
    "print(\"X\",X)\n",
    "print(\"y\",y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-virginica']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Step 1: Label-encode data set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "encoded_y = label_encoder.transform(y)\n",
    "print(y)\n",
    "print(encoded_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-setosa\n",
      "Encoded Label: 0\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-versicolor\n",
      "Encoded Label: 1\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n",
      "Original Class: Iris-virginica\n",
      "Encoded Label: 2\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "for label, original_class in zip(encoded_y, y):\n",
    "    print('Original Class: ' + str(original_class))\n",
    "    print('Encoded Label: ' + str(label))\n",
    "    print('-' * 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each of the original labels has been replaced with an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy.core._multiarray_umath'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy.core._multiarray_umath'"
     ]
    },
    {
     "ename": "SystemError",
     "evalue": "<class '_frozen_importlib._ModuleLockManager'> returned a result with an error set",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mSystemError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mSystemError\u001b[0m: <class '_frozen_importlib._ModuleLockManager'> returned a result with an error set"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core._multiarray_umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: numpy.core._multiarray_umath failed to import"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.umath failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.umath failed to import"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Step 2: One-hot encoding\n",
    "one_hot_y = to_categorical(encoded_y)\n",
    "one_hot_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot Encoding converts that numeric value to a one-hot encoded array. One-Hot Encoding avoids biasing the model by applying numeric classes.\n",
    "\n",
    "https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everyone Activity 2 Neural Networks with Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some fake data with 3 features\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_features=3, n_redundant=0, n_informative=3,\n",
    "                           random_state=42, n_classes=2, n_clusters_per_class=1)\n",
    "\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use train_test_split to create training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "It is really important to scale our data before using multilayer perceptron models. \n",
    "\n",
    "Without scaling, it is often difficult for the training cycle to converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaler = StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to scale both the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# One-hot encoding\n",
    "y_train_categorical = to_categorical(y_train)\n",
    "y_test_categorical = to_categorical(y_test)\n",
    "y_train_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our Model\n",
    "\n",
    "We must first decide what kind of model to apply to our data. \n",
    "\n",
    "For numerical data, we use a regressor model. \n",
    "\n",
    "For categorical data, we use a classifier model. \n",
    "\n",
    "In this example, we will use a classifier to build the following network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nnet.png](Images/nnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining our Model Architecture (the layers)\n",
    "\n",
    "We first need to create a sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we add our first layer. This layer requires you to specify both the number of inputs and the number of nodes that you want in the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "number_inputs = 3\n",
    "number_hidden_nodes = 4\n",
    "model.add(Dense(units=number_hidden_nodes,\n",
    "                activation='relu', input_dim=number_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![first_layer](Images/nnet_first_layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final layer is the output layer. Here, we need to specify the activation function (typically `softmax` for classification) and the number of classes (labels) that we are trying to predict (2 in this example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_classes = 2\n",
    "model.add(Dense(units=number_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![output_layer](Images/nnet_output_layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the Model\n",
    "\n",
    "Now that we have our model architecture defined, we must compile the model using a loss function and optimizer. We can also specify additional training metrics such as accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use categorical crossentropy for categorical data and mean squared error for regression\n",
    "# Hint: your output layer in this example is using software for logistic regression (categorical)\n",
    "# If your output layer activation was `linear` then you may want to use `mse` for loss\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "Finally, we train our model using our training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training consists of updating our weights using our optimizer and loss function. In this example, we choose 1000 iterations (loops) of training that are called epochs.\n",
    "\n",
    "We also choose to shuffle our training data and increase the detail printed out during each training cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit (train) the model\n",
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=1000,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantifying the Model\n",
    "We use our testing data to validate our model. This is how we determine the validity of our model (i.e. the ability to predict new and previously unseen data points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the testing data\n",
    "model_loss, model_accuracy = model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions with new data\n",
    "\n",
    "We can use our trained model to make predictions using `model.predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "new_data = np.array([[0.2, 0.3, 0.4]])\n",
    "print(f\"Predicted class: {model.predict_classes(new_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everyone Do Activity 3 Deep Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "from numpy.random import seed\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Data\n",
    "X, y = sklearn.datasets.make_circles(noise=0.05, factor=.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x11b572908>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmZnl/AOXGvHAEXhOxQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize Data\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use train_test_split to create training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a StandardScater model and fit it to the training data\n",
    "X_scaler = StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform the training and testing data using the X_scaler\n",
    "\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# One-hot encoding\n",
    "y_train_categorical = to_categorical(y_train)\n",
    "y_test_categorical = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first, create a normal neural network with 2 inputs, 6 hidden nodes, and 2 outputs\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=6, activation='relu', input_dim=2))\n",
    "model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 6)                 18        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 14        \n",
      "=================================================================\n",
      "Total params: 32\n",
      "Trainable params: 32\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 0s - loss: 0.5680 - acc: 0.6667\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.5669 - acc: 0.6667\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.5657 - acc: 0.6667\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.5647 - acc: 0.6667\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.5635 - acc: 0.6667\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.5624 - acc: 0.6667\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.5612 - acc: 0.6800\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.5602 - acc: 0.6933\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.5591 - acc: 0.7067\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.5580 - acc: 0.7067\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.5568 - acc: 0.7067\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.5558 - acc: 0.7200\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.5547 - acc: 0.7200\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.5537 - acc: 0.7200\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.5525 - acc: 0.7200\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.5514 - acc: 0.7200\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.5503 - acc: 0.7333\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.5492 - acc: 0.7333\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.5481 - acc: 0.7333\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.5470 - acc: 0.7333\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.5459 - acc: 0.7333\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.5448 - acc: 0.7600\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.5437 - acc: 0.7600\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.5426 - acc: 0.7733\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.5415 - acc: 0.7733\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.5404 - acc: 0.7733\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.5393 - acc: 0.7733\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.5381 - acc: 0.7733\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.5370 - acc: 0.7733\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.5359 - acc: 0.7733\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.5348 - acc: 0.7867\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.5338 - acc: 0.7867\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.5327 - acc: 0.7867\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.5315 - acc: 0.7867\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.5303 - acc: 0.7867\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.5293 - acc: 0.7867\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.5281 - acc: 0.7867\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.5268 - acc: 0.7867\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.5257 - acc: 0.7867\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.5246 - acc: 0.7867\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.5233 - acc: 0.7867\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.5222 - acc: 0.8000\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.5210 - acc: 0.8000\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.5197 - acc: 0.8000\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.5185 - acc: 0.8000\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.5174 - acc: 0.8133\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.5161 - acc: 0.8267\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.5148 - acc: 0.8267\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.5136 - acc: 0.8267\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.5125 - acc: 0.8267\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.5112 - acc: 0.8267\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.5101 - acc: 0.8267\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.5089 - acc: 0.8267\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.5077 - acc: 0.8267\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.5065 - acc: 0.8400\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.5054 - acc: 0.8400\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.5041 - acc: 0.8400\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.5028 - acc: 0.8400\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.5015 - acc: 0.8400\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.5003 - acc: 0.8400\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.4990 - acc: 0.8400\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.4977 - acc: 0.8400\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.4965 - acc: 0.8400\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.4951 - acc: 0.8400\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.4939 - acc: 0.8400\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.4925 - acc: 0.8400\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.4912 - acc: 0.8400\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.4899 - acc: 0.8400\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.4885 - acc: 0.8400\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.4873 - acc: 0.8400\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.4858 - acc: 0.8400\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.4845 - acc: 0.8400\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.4831 - acc: 0.8400\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.4818 - acc: 0.8400\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.4805 - acc: 0.8400\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.4790 - acc: 0.8400\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.4777 - acc: 0.8400\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.4763 - acc: 0.8400\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.4749 - acc: 0.8533\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.4734 - acc: 0.8533\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.4720 - acc: 0.8533\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.4706 - acc: 0.8533\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.4692 - acc: 0.8533\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.4677 - acc: 0.8667\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.4663 - acc: 0.8667\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.4650 - acc: 0.8667\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.4635 - acc: 0.8667\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.4622 - acc: 0.8667\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.4608 - acc: 0.8667\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.4594 - acc: 0.8667\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.4579 - acc: 0.8667\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.4566 - acc: 0.8667\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.4553 - acc: 0.8667\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.4536 - acc: 0.8667\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.4521 - acc: 0.8667\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.4506 - acc: 0.8667\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.4490 - acc: 0.8800\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.4475 - acc: 0.8800\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.4458 - acc: 0.8800\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.4443 - acc: 0.8800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x133750e48>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model to the training data\n",
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=100,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "For this network, we simply add an additional hidden layer of 6 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deep_model = Sequential()\n",
    "deep_model.add(Dense(units=6, activation='relu', input_dim=2))\n",
    "deep_model.add(Dense(units=6, activation='relu'))\n",
    "deep_model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 6)                 18        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 14        \n",
      "=================================================================\n",
      "Total params: 74\n",
      "Trainable params: 74\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "deep_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 0s - loss: 0.6183 - acc: 0.5067\n",
      "Epoch 2/100\n",
      " - 0s - loss: 0.6154 - acc: 0.5333\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.6126 - acc: 0.5333\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.6101 - acc: 0.5333\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.6075 - acc: 0.5333\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.6047 - acc: 0.5333\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.6021 - acc: 0.5333\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.5992 - acc: 0.5333\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.5966 - acc: 0.5467\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.5938 - acc: 0.5600\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.5910 - acc: 0.5733\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.5885 - acc: 0.5733\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.5856 - acc: 0.5733\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.5828 - acc: 0.5867\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.5799 - acc: 0.5867\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.5772 - acc: 0.6133\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.5740 - acc: 0.6133\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.5709 - acc: 0.6133\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.5677 - acc: 0.6267\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.5643 - acc: 0.6667\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.5607 - acc: 0.7067\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.5572 - acc: 0.7200\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.5538 - acc: 0.7200\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.5503 - acc: 0.7467\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.5468 - acc: 0.7733\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.5434 - acc: 0.7733\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.5400 - acc: 0.7867\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.5367 - acc: 0.8000\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.5334 - acc: 0.8000\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.5302 - acc: 0.8000\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.5268 - acc: 0.8000\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.5235 - acc: 0.8133\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.5202 - acc: 0.8400\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.5168 - acc: 0.8533\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.5134 - acc: 0.8533\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.5100 - acc: 0.8533\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.5066 - acc: 0.8667\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.5032 - acc: 0.8667\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.4998 - acc: 0.8667\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.4964 - acc: 0.8800\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.4930 - acc: 0.9067\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.4895 - acc: 0.9200\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.4861 - acc: 0.9200\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.4825 - acc: 0.9333\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.4789 - acc: 0.9600\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.4755 - acc: 0.9600\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.4717 - acc: 0.9600\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.4680 - acc: 0.9600\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.4642 - acc: 0.9600\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.4604 - acc: 0.9733\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.4565 - acc: 0.9733\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.4526 - acc: 0.9733\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.4488 - acc: 0.9733\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.4450 - acc: 0.9733\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.4413 - acc: 0.9733\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.4373 - acc: 0.9733\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.4337 - acc: 0.9733\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.4298 - acc: 0.9733\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.4260 - acc: 0.9733\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.4224 - acc: 0.9733\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.4188 - acc: 0.9733\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.4150 - acc: 0.9733\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.4115 - acc: 0.9867\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.4080 - acc: 0.9867\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.4044 - acc: 0.9867\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.4009 - acc: 0.9867\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.3975 - acc: 0.9867\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.3938 - acc: 0.9867\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.3903 - acc: 1.0000\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.3869 - acc: 1.0000\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.3834 - acc: 1.0000\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.3801 - acc: 1.0000\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.3766 - acc: 1.0000\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.3733 - acc: 1.0000\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.3700 - acc: 1.0000\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.3668 - acc: 1.0000\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.3635 - acc: 1.0000\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.3603 - acc: 1.0000\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.3572 - acc: 1.0000\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.3540 - acc: 1.0000\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.3508 - acc: 1.0000\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.3477 - acc: 1.0000\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.3447 - acc: 1.0000\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.3417 - acc: 1.0000\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.3387 - acc: 1.0000\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.3357 - acc: 1.0000\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.3328 - acc: 1.0000\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.3299 - acc: 1.0000\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.3270 - acc: 1.0000\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.3242 - acc: 1.0000\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.3215 - acc: 1.0000\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.3187 - acc: 1.0000\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.3160 - acc: 1.0000\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.3132 - acc: 1.0000\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.3105 - acc: 1.0000\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.3077 - acc: 1.0000\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.3051 - acc: 1.0000\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.3024 - acc: 1.0000\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.2997 - acc: 1.0000\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.2970 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13379ae48>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_model.compile(optimizer='adam',\n",
    "                   loss='categorical_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "deep_model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=100,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the models below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Neural Network - Loss: 0.5199212431907654, Accuracy: 0.800000011920929\n"
     ]
    }
   ],
   "source": [
    "model_loss, model_accuracy = model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Neural Network - Loss: 0.3295448422431946, Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "model_loss, model_accuracy = deep_model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(f\"Deep Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In a nutshell..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stack more layers](Images/stack-more-layers.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Turn Activity 4 Moon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Deep Learning Challenge\n",
    "\n",
    "In this activity, you will create a regular Neural Network and a Deep Neural Network. Then, compare the accuracy of each.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Create a Neural Network and Deep Neural Network Classifier that correctly classifies both moons from the dataset.\n",
    "\n",
    "* Train both models using 100 training epochs.\n",
    "\n",
    "* Compare the accuracy of both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "from numpy.random import seed\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = sklearn.datasets.make_moons(200, noise=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use train_test_split to create training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a StandardScater model and fit it to the training data\n",
    "X_scaler = StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform the training and testing data using the X_scaler\n",
    "\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# One-hot encoding\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, create a normal neural network with 2 inputs, 6 hidden nodes, and 2 outputs\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit the model to the training data\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "For this network, we simply add an additional hidden layer of 6 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a deep learning model with an extra hidden layer of 6 nodes called `deep_model`\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deep_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile and fit the deep_model\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the models below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_loss, model_accuracy = model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_loss, model_accuracy = deep_model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(f\"Deep Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Turn Activity 5 Deep Voice "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice Gender Recognition\n",
    "\n",
    "* In this activity, you will apply a deep learning neural network to predict the gender of a voice using acoustic properties of the voice and speech.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Create a deep learning model with 2 hidden layers.  Each layer should have 100 nodes.\n",
    "\n",
    "* Compile and fit the model.\n",
    "\n",
    "* Quantify (score) the model.\n",
    "\n",
    "* Use the first 5 testing data points to make predictions.  Then, compare the predictions to the actual labels.\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice Gender\n",
    "Gender Recognition by Voice and Speech Analysis\n",
    "\n",
    "This database was created to identify a voice as male or female, based upon acoustic properties of the voice and speech. The dataset consists of 3,168 recorded voice samples, collected from male and female speakers. The voice samples are pre-processed by acoustic analysis in R using the seewave and tuneR packages, with an analyzed frequency range of 0hz-280hz (human vocal range).\n",
    "\n",
    "## The Dataset\n",
    "The following acoustic properties of each voice are measured and included within the CSV:\n",
    "\n",
    "* meanfreq: mean frequency (in kHz)\n",
    "* sd: standard deviation of frequency\n",
    "* median: median frequency (in kHz)\n",
    "* Q25: first quantile (in kHz)\n",
    "* Q75: third quantile (in kHz)\n",
    "* IQR: interquantile range (in kHz)\n",
    "* skew: skewness (see note in specprop description)\n",
    "* kurt: kurtosis (see note in specprop description)\n",
    "* sp.ent: spectral entropy\n",
    "* sfm: spectral flatness\n",
    "* mode: mode frequency\n",
    "* centroid: frequency centroid (see specprop)\n",
    "* peakf: peak frequency (frequency with highest energy)\n",
    "* meanfun: average of fundamental frequency measured across acoustic signal\n",
    "* minfun: minimum fundamental frequency measured across acoustic signal\n",
    "* maxfun: maximum fundamental frequency measured across acoustic signal\n",
    "* meandom: average of dominant frequency measured across acoustic signal\n",
    "* mindom: minimum of dominant frequency measured across acoustic signal\n",
    "* maxdom: maximum of dominant frequency measured across acoustic signal\n",
    "* dfrange: range of dominant frequency measured across acoustic signal\n",
    "* modindx: modulation index. Calculated as the accumulated absolute difference between adjacent measurements of fundamental frequencies divided by the frequency range\n",
    "* label: male or female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meanfreq</th>\n",
       "      <th>sd</th>\n",
       "      <th>median</th>\n",
       "      <th>Q25</th>\n",
       "      <th>Q75</th>\n",
       "      <th>IQR</th>\n",
       "      <th>skew</th>\n",
       "      <th>kurt</th>\n",
       "      <th>sp.ent</th>\n",
       "      <th>sfm</th>\n",
       "      <th>...</th>\n",
       "      <th>centroid</th>\n",
       "      <th>meanfun</th>\n",
       "      <th>minfun</th>\n",
       "      <th>maxfun</th>\n",
       "      <th>meandom</th>\n",
       "      <th>mindom</th>\n",
       "      <th>maxdom</th>\n",
       "      <th>dfrange</th>\n",
       "      <th>modindx</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.059781</td>\n",
       "      <td>0.064241</td>\n",
       "      <td>0.032027</td>\n",
       "      <td>0.015071</td>\n",
       "      <td>0.090193</td>\n",
       "      <td>0.075122</td>\n",
       "      <td>12.863462</td>\n",
       "      <td>274.402906</td>\n",
       "      <td>0.893369</td>\n",
       "      <td>0.491918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059781</td>\n",
       "      <td>0.084279</td>\n",
       "      <td>0.015702</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.066009</td>\n",
       "      <td>0.067310</td>\n",
       "      <td>0.040229</td>\n",
       "      <td>0.019414</td>\n",
       "      <td>0.092666</td>\n",
       "      <td>0.073252</td>\n",
       "      <td>22.423285</td>\n",
       "      <td>634.613855</td>\n",
       "      <td>0.892193</td>\n",
       "      <td>0.513724</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066009</td>\n",
       "      <td>0.107937</td>\n",
       "      <td>0.015826</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.009014</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.077316</td>\n",
       "      <td>0.083829</td>\n",
       "      <td>0.036718</td>\n",
       "      <td>0.008701</td>\n",
       "      <td>0.131908</td>\n",
       "      <td>0.123207</td>\n",
       "      <td>30.757155</td>\n",
       "      <td>1024.927705</td>\n",
       "      <td>0.846389</td>\n",
       "      <td>0.478905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077316</td>\n",
       "      <td>0.098706</td>\n",
       "      <td>0.015656</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.007990</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.072111</td>\n",
       "      <td>0.158011</td>\n",
       "      <td>0.096582</td>\n",
       "      <td>0.207955</td>\n",
       "      <td>0.111374</td>\n",
       "      <td>1.232831</td>\n",
       "      <td>4.177296</td>\n",
       "      <td>0.963322</td>\n",
       "      <td>0.727232</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151228</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.017798</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.201497</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.247119</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.135120</td>\n",
       "      <td>0.079146</td>\n",
       "      <td>0.124656</td>\n",
       "      <td>0.078720</td>\n",
       "      <td>0.206045</td>\n",
       "      <td>0.127325</td>\n",
       "      <td>1.101174</td>\n",
       "      <td>4.333713</td>\n",
       "      <td>0.971955</td>\n",
       "      <td>0.783568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135120</td>\n",
       "      <td>0.106398</td>\n",
       "      <td>0.016931</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.712812</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>5.484375</td>\n",
       "      <td>5.476562</td>\n",
       "      <td>0.208274</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   meanfreq        sd    median       Q25       Q75       IQR       skew  \\\n",
       "0  0.059781  0.064241  0.032027  0.015071  0.090193  0.075122  12.863462   \n",
       "1  0.066009  0.067310  0.040229  0.019414  0.092666  0.073252  22.423285   \n",
       "2  0.077316  0.083829  0.036718  0.008701  0.131908  0.123207  30.757155   \n",
       "3  0.151228  0.072111  0.158011  0.096582  0.207955  0.111374   1.232831   \n",
       "4  0.135120  0.079146  0.124656  0.078720  0.206045  0.127325   1.101174   \n",
       "\n",
       "          kurt    sp.ent       sfm  ...    centroid   meanfun    minfun  \\\n",
       "0   274.402906  0.893369  0.491918  ...    0.059781  0.084279  0.015702   \n",
       "1   634.613855  0.892193  0.513724  ...    0.066009  0.107937  0.015826   \n",
       "2  1024.927705  0.846389  0.478905  ...    0.077316  0.098706  0.015656   \n",
       "3     4.177296  0.963322  0.727232  ...    0.151228  0.088965  0.017798   \n",
       "4     4.333713  0.971955  0.783568  ...    0.135120  0.106398  0.016931   \n",
       "\n",
       "     maxfun   meandom    mindom    maxdom   dfrange   modindx  label  \n",
       "0  0.275862  0.007812  0.007812  0.007812  0.000000  0.000000   male  \n",
       "1  0.250000  0.009014  0.007812  0.054688  0.046875  0.052632   male  \n",
       "2  0.271186  0.007990  0.007812  0.015625  0.007812  0.046512   male  \n",
       "3  0.250000  0.201497  0.007812  0.562500  0.554688  0.247119   male  \n",
       "4  0.266667  0.712812  0.007812  5.484375  5.476562  0.208274   male  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voice = pd.read_csv('Resources/voice.csv')\n",
    "voice.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = voice.drop(\"label\", axis=1)\n",
    "y = voice[\"label\"]\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)\n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Step 1: Label-encode data set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "encoded_y_train = label_encoder.transform(y_train)\n",
    "encoded_y_test = label_encoder.transform(y_test)\n",
    "\n",
    "# Step 2: Convert encoded labels to one-hot-encoding\n",
    "y_train_categorical = to_categorical(encoded_y_train)\n",
    "y_test_categorical = to_categorical(encoded_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Create a Neural Network model here\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile and fit the model\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantify our Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_loss, model_accuracy = model.evaluate(X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the first 5 test data values to make a prediction and compare it to the actual labels\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "print(f\"Actual Labels: {list(y_test[:5])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructor Turn Activity 6 Saving Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3168, 20) (3168,)\n"
     ]
    }
   ],
   "source": [
    "voice = pd.read_csv('Resources/voice.csv')\n",
    "\n",
    "X = voice.drop(\"label\", axis=1)\n",
    "y = voice[\"label\"]\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=1, stratify=y)\n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Step 1: Label-encode data set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "encoded_y_train = label_encoder.transform(y_train)\n",
    "encoded_y_test = label_encoder.transform(y_test)\n",
    "\n",
    "# Step 2: Convert encoded labels to one-hot-encoding\n",
    "y_train_categorical = to_categorical(encoded_y_train)\n",
    "y_test_categorical = to_categorical(encoded_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=100, activation='relu', input_dim=20))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x133e79438>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=60,\n",
    "    shuffle=True,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantify our Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Neural Network - Loss: 0.057270565901669186, Accuracy: 0.9861111111111112\n"
     ]
    }
   ],
   "source": [
    "model_loss, model_accuracy = model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving a Trained Model\n",
    "We can save our trained models using the HDF5 binary format with the extension `.h5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"voice_model_trained.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from keras.models import load_model\n",
    "voice_model = load_model(\"voice_model_trained.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Neural Network - Loss: 0.057270565901669186, Accuracy: 0.9861111111111112\n"
     ]
    }
   ],
   "source": [
    "model_loss, model_accuracy = voice_model.evaluate(\n",
    "    X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Turn Activity 7 Smart Phones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Phone Activity Detector\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Follow the comments in the provided starter file to:\n",
    "\n",
    "  * Encode necessary labels.\n",
    "\n",
    "  * Build and train a deep learning model.\n",
    "\n",
    "  * Save the model.\n",
    "\n",
    "  * Load to model.\n",
    "\n",
    "  * Use the loaded model to predict the activity of a smartphone user based one data point from the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3168, 20) (3168,)\n"
     ]
    }
   ],
   "source": [
    "voice = pd.read_csv('Resources/voice.csv')\n",
    "\n",
    "X = voice.drop(\"label\", axis=1)\n",
    "y = voice[\"label\"]\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)\n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Step 1: Label-encode data set\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "encoded_y_train = label_encoder.transform(y_train)\n",
    "encoded_y_test = label_encoder.transform(y_test)\n",
    "\n",
    "# Step 2: Convert encoded labels to one-hot-encoding\n",
    "y_train_categorical = to_categorical(encoded_y_train)\n",
    "y_test_categorical = to_categorical(encoded_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=100, activation='relu', input_dim=20))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=60,\n",
    "    shuffle=True,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantify our Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_loss, model_accuracy = model.evaluate(X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving a Trained Model\n",
    "We can save our trained models using the HDF5 binary format with the extension `.h5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructor Turn Activity 8 Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate 4 clusters of random data.\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "data, _ = make_blobs(n_samples=300, centers=4,\n",
    "                     cluster_std=0.60, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.scatter(data[:, 0], data[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use n_clusters=4 as the k value\n",
    "# We can see from the plot above that there are 4 clusters\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit the model to the data\n",
    "kmeans.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict the clusters\n",
    "predicted_clusters = kmeans.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the predicted clusters to see if the model predicted the correct clusters\n",
    "# This is visual validation that the model was trained correctly.\n",
    "plt.scatter(data[:, 0], data[:, 1], c=predicted_clusters, s=50, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Turn Activity 9 Kmeans "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Use the starter code to fit a kmeans model to a dataset.\n",
    "\n",
    "* Visualize the results by creating a plot that looks like the following.\n",
    "\n",
    "  ![Stu_Kmeans_plot](Images/Stu_Kmeans_plot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create dummy data\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "X, _ = make_blobs(n_samples=500, centers=6,\n",
    "                            cluster_std=0.70, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot and show scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a kmeans model using k = 12\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# Create a kmeans model using k = 12\n",
    "\n",
    "\n",
    "# Use the data to predict the clusters\n",
    "# save the predictions as `predicted_clusters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the clusters\n",
    "plt.scatter(X[:, 0], X[:, 1], c=predicted_clusters, s=50, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bonus\n",
    "# Print the cluster centers and cluster labels\n",
    "# YOUR CODE HERE\n",
    "centers = kmeans.cluster_centers_\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis')\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
