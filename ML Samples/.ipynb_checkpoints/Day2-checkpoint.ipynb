{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21.2 Classification\n",
    "\n",
    "\n",
    "### Class Objectives\n",
    "\n",
    "* Will understand how to calculate and apply the fundamental classification algorithms: logistic regression, SVM, KNN, decision trees, and random forests.\n",
    "\n",
    "* Will understand how to quantify and validate classification models including calculating a classification report.\n",
    "\n",
    "* Will understand how to apply `GridSearchCV` to hyper tune model parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Instructor Turn Activity 1 Logistic Regression\n",
    "\n",
    "Logistic Regression is a statistical method for predicting binary outcomes from data.\n",
    "\n",
    "Examples of this are \"yes\" vs \"no\" or \"young\" vs \"old\". \n",
    "\n",
    "These are categories that translate to probability of being a 0 or a 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate logistic regression by adding an activation function as the final step to our linear model. \n",
    "\n",
    "This converts the linear regression output to a probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  * Logistic Regression is a statistical method for predicting binary outcomes from data. With linear regression, our linear model may provide a numerical output such as age. With logistic regression, the numerical value for age could be translated to a probability between 0 and 1. This discrete output could then be labeled as \"young\" vs \"old\".\n",
    "\n",
    "    ![logistic-regression.png](Images/logistic-regression.png)\n",
    "\n",
    "  * Logistic regression is calculated by applying an activation function as the final step to our linear model. This transforms a numerical range to a bounded probability between 0 and 1.\n",
    "\n",
    "    ![logistic-regression-activation-function.png](Images/logistic-regression-activation-function.png)\n",
    "\n",
    "  * We can use logistic regression to predict which category or class a new data point should have.\n",
    "\n",
    "    ![logistic_1.png](Images/logistic_1.png)\n",
    "    ![logistic_2.png](Images/logistic_2.png)\n",
    "    ![logistic_3.png](Images/logistic_3.png)\n",
    "    ![logistic_4.png](Images/logistic_4.png)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `make_blobs` function to generate two different groups (classes) of data. We can then apply logistic regression to determine if new data points belong to the purple group or the yellow group.\n",
    "\n",
    "    ![make-blobs.png](Images/make-blobs.png)\n",
    "\n",
    "  * We create our model using the `LogisticRegression` class from Sklearn.\n",
    "\n",
    "    ![logistic-regression-model.png](Images/logistic-regression-model.png)\n",
    "\n",
    "  * Then we fit (train) the model using our training data.\n",
    "\n",
    "    ![train-logistic-model.png](Images/train-logistic-model.png)\n",
    "\n",
    "  * And validate it using the test data.\n",
    "\n",
    "    ![test-logistic-model.png](Images/test-logistic-model.png)\n",
    "\n",
    "  * And finally, we can make predictions.\n",
    "\n",
    "    ![new-data.png](Images/new-data.png)\n",
    "\n",
    "    ![predicted-class.png](Images/predicted-class.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_blobs Generate isotropic Gaussian blobs for clustering.\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(centers=2, random_state=42)\n",
    "\n",
    "print(f\"Labels: {y[:10]}\")\n",
    "print(f\"Data: {X[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing both classes\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split our data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit (train) or model using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate the model using the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training Data Score: {classifier.score(X_train, y_train)}\")\n",
    "print(f\"Testing Data Score: {classifier.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new data point (the red circle)\n",
    "import numpy as np\n",
    "new_data = np.array([[-0, 6]])\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.scatter(new_data[0, 0], new_data[0, 1], c=\"r\", marker=\"o\", s=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class (purple or yellow) of the new data point\n",
    "predictions = classifier.predict(new_data)\n",
    "print(\"Classes are either 0 (purple) or 1 (yellow)\")\n",
    "print(f\"The new point was classified as: {predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(X_test)\n",
    "pd.DataFrame({\"Prediction\": predictions, \"Actual\": y_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Students Turn Activity 2 Voice Gender Recognition\n",
    "\n",
    "* In this activity, you will apply logistic regression to predict the gender of a voice using acoustic properties of the voice and speech.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Split your data into training and testing data.\n",
    "\n",
    "* Create a logistic regression model with sklearn.\n",
    "\n",
    "* Fit the model to the training data.\n",
    "\n",
    "* Make 10 predictions and compare those to the testing data labels.\n",
    "\n",
    "* Compute the R2 score for the training and testing data separately.\n",
    "\n",
    "- - -\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voice Gender\n",
    "Gender Recognition by Voice and Speech Analysis\n",
    "\n",
    "This database was created to identify a voice as male or female, based upon acoustic properties of the voice and speech. The dataset consists of 3,168 recorded voice samples, collected from male and female speakers. The voice samples are pre-processed by acoustic analysis in R using the seewave and tuneR packages, with an analyzed frequency range of 0hz-280hz (human vocal range).\n",
    "\n",
    "## The Dataset\n",
    "The following acoustic properties of each voice are measured and included within the CSV:\n",
    "\n",
    "* meanfreq: mean frequency (in kHz)\n",
    "* sd: standard deviation of frequency\n",
    "* median: median frequency (in kHz)\n",
    "* Q25: first quantile (in kHz)\n",
    "* Q75: third quantile (in kHz)\n",
    "* IQR: interquantile range (in kHz)\n",
    "* skew: skewness (see note in specprop description)\n",
    "* kurt: kurtosis (see note in specprop description)\n",
    "* sp.ent: spectral entropy\n",
    "* sfm: spectral flatness\n",
    "* mode: mode frequency\n",
    "* centroid: frequency centroid (see specprop)\n",
    "* peakf: peak frequency (frequency with highest energy)\n",
    "* meanfun: average of fundamental frequency measured across acoustic signal\n",
    "* minfun: minimum fundamental frequency measured across acoustic signal\n",
    "* maxfun: maximum fundamental frequency measured across acoustic signal\n",
    "* meandom: average of dominant frequency measured across acoustic signal\n",
    "* mindom: minimum of dominant frequency measured across acoustic signal\n",
    "* maxdom: maximum of dominant frequency measured across acoustic signal\n",
    "* dfrange: range of dominant frequency measured across acoustic signal\n",
    "* modindx: modulation index. Calculated as the accumulated absolute difference between adjacent measurements of fundamental frequencies divided by the frequency range\n",
    "* label: male or female"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Logistic regression is used to predict categories or labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voice = pd.read_csv(os.path.join('Resources', 'voice.csv'))\n",
    "voice.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign X (data) and y (target)\n",
    "\n",
    "X = voice.drop(\"label\", axis=1)\n",
    "y = voice[\"label\"]\n",
    "print(X.shape, y.shape)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Split our data into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data using train_test_split\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Create a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a logistic regression model\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Fit (train) or model using the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit the model to the data\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Validate the model using the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the r2 score for the test data\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions using the X_test and y_test data\n",
    "# Print at least 10 predictions vs their actual labels\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanations: \n",
    "  * The `stratify` parameter in `train_test_split` to obtain a representative sample of each category in our test data.\n",
    "    ![statify.png](Images/stratify.png)\n",
    "\n",
    "  * We will perform logistic regression to our dataset in order to predict the label `male` or `female`.\n",
    "\n",
    "    ![gender-predictions.png](Images/gender-predictions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructor Turn Activity 3 Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Decision Trees encode a series of True/False questions that can be interpreted as if-else statements\n",
    "\n",
    "    ![decision-tree.png](Images/decision-tree.png)\n",
    "\n",
    "    ![dtree-ifelse.png](Images/dtree-ifelse.png)\n",
    "\n",
    "  * Decision trees have a depth: the number of `if-else` statements encountered before making a decision.\n",
    "\n",
    "  * Decision trees can become very complex and very deep, depending on how many questions have to be answered. Deep and complex trees tend to overfit to the data and do not generalize well.\n",
    "\n",
    "    ![tree.png](Images/tree.png)\n",
    "\n",
    "* Random Forests:\n",
    "\n",
    "  * Instead of one large, complex tree, you use many small and simple decision trees and average their outputs.\n",
    "\n",
    "  * These simple trees are created by randomly sampling the data and creating a decision tree for only that small portion of data. This is known as a **weak classifier** because it is only trained on a small piece of the original data and by itself is only slightly better than a random guess. However, many \"slightly better than average\" small decision trees can be combined to create a **strong classifier**, which has much better decision making power.\n",
    "\n",
    "  * Another benefit to this algorithm is that it is robust against overfitting. This is because all of those weak classifiers are trained on different pieces of the data.\n",
    "\n",
    "    ![random-forest.png](Images/random-forest.png)\n",
    "\n",
    "* Each node in the tree attempts to split the data based on some criteria of the input data. The top of the tree will be the decision point that makes the biggest split. Each sub-node makes a finer and finer grain decision as the depth increases.\n",
    "\n",
    "  ![iris.png](Images/iris.png)\n",
    "\n",
    "* Point out that the training phase of the decision tree algorithm learns which features best split the data.\n",
    "\n",
    "* Explain a byproduct of the Random Forest algorithm is a ranking of feature importance (i.e. which features have the most impact on the decision).\n",
    "\n",
    "* Scikit-Learn provides an attribute called `feature_importances_`, where you can see which features were the most significant.\n",
    "\n",
    "  ```python\n",
    "  sorted(zip(rf.feature_importances_, iris.feature_names), reverse=True)\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris Dataset\n",
    "iris = load_iris()\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=200)\n",
    "rf = rf.fit(iris.data, iris.target)\n",
    "rf.score(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forests in sklearn will automatically calculate feature importance\n",
    "importances = rf.feature_importances_\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can sort the features by their importance\n",
    "sorted(zip(rf.feature_importances_, iris.feature_names), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Students Turn Activity # Trees\n",
    "\n",
    "* In this activity, you will compare the performance of a decision tree to a random forest classifier using the Pima Diabetes DataSet.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Use the Pima Diabetes DataSet and train a decision tree classifier to predict the diabetes label (positive or negative). Print the score for the trained model using the test data.\n",
    "\n",
    "* Repeat the exercise using a Random Forest Classifier with SciKit-Learn. You will need to investigate the SciKit-Learn documentation to determine how to build and train this model.\n",
    "\n",
    "* Experiment with different numbers of estimators in your random forest model. Try different values between 100 and 1000 and compare the scores.\n",
    "\n",
    "- - -\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(\"Resources\", \"diabetes.csv\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = df[\"Outcome\"]\n",
    "target_names = [\"negative\", \"positive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = df.drop(\"Outcome\", axis=1)\n",
    "feature_names = data.columns\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data using train_test_split\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Decision Tree Classifier\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit the classifier to the data\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the R2 score for the test data\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bonus\n",
    "# Create, fit, and score a Random Forest Classifier\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * The accuracy improves slightly when using a random forest classifier. Change the number of estimators in the random forest model and re-compute the score to show how it changes.\n",
    "\n",
    "    ![nestimators.png](Images/nestimators.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructor Turn Activity KNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The KNN algorithm is a simple, yet robust machine learning algorithm. It can be used for both regression and classification. However, it is typically used for classification.\n",
    "\n",
    "  * Walk through the examples provided and show how K changes the classification. we use odd numbers for k so that there isn't a tie between neighboring points.\n",
    "\n",
    "    ![k1.png](Images/k1.png)\n",
    "\n",
    "    ![k3.png](Images/k3.png)\n",
    "\n",
    "    ![k5.png](Images/k5.png)\n",
    "\n",
    "    ![k7.png](Images/k7.png)\n",
    "\n",
    "  * Finally, the `k` for KNN is often calculated computationally with a loop.\n",
    "\n",
    "  * Point out that the best `k` value for this dataset is where the score is both accurate and has started to stabilize.\n",
    "\n",
    "    ![knn-scores.png](Images/knn-scores.png)\n",
    "\n",
    "    ![knn-plot.png](Images/knn-plot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a StandardScater model and fit it to the training data\n",
    "\n",
    "X_scaler = StandardScaler().fit(X_train.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform the training and testing data using the X_scaler and y_scaler models\n",
    "\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through different k values to see which has the highest accuracy\n",
    "# Note: We only use odd numbers because we don't want any ties\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for k in range(1, 20, 2):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    train_score = knn.score(X_train_scaled, y_train)\n",
    "    test_score = knn.score(X_test_scaled, y_test)\n",
    "    train_scores.append(train_score)\n",
    "    test_scores.append(test_score)\n",
    "    print(f\"k: {k}, Train/Test Score: {train_score:.3f}/{test_score:.3f}\")\n",
    "    \n",
    "    \n",
    "plt.plot(range(1, 20, 2), train_scores, marker='o', label='training')\n",
    "plt.plot(range(1, 20, 2), test_scores, marker=\"x\", label='testing')\n",
    "plt.legend()\n",
    "plt.xlabel(\"k neighbors\")\n",
    "plt.ylabel(\"Testing accuracy Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that k: 9 provides the best accuracy where the classifier starts to stablize\n",
    "knn = KNeighborsClassifier(n_neighbors=9)\n",
    "knn.fit(X_train, y_train)\n",
    "print('k=9 Test Acc: %.3f' % knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_iris_data = [[4.3, 3.2, 1.3, 0.2]]\n",
    "predicted_class = knn.predict(new_iris_data)\n",
    "print(predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Students Turn KNN Activity \n",
    "\n",
    "* In this activity, you will determine the best `k` value in KNN to predict diabetes for the Pima Diabetes DataSet.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Calculate the Train and Test scores for `k` ranging from 1 to 20. Use only odd numbers for the k values.\n",
    "\n",
    "* Plot the `k` values for both the train and test data to determine where the best combination of scores occur. This point will be the optimal `k` value for your model.\n",
    "\n",
    "* Re-train your model using the `k` value that you found to have the best scores. Print the score for this value.\n",
    "\n",
    "- - -\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(\"Resources\", \"diabetes 2.csv\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = df[\"Outcome\"]\n",
    "target_names = [\"negative\", \"positive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop(\"Outcome\", axis=1)\n",
    "feature_names = data.columns\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through different k values to see which has the highest accuracy\n",
    "# Note: We only use odd numbers because we don't want any ties\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "    \n",
    "plt.plot(range(1, 20, 2), train_scores, marker='o')\n",
    "plt.plot(range(1, 20, 2), test_scores, marker=\"x\")\n",
    "plt.xlabel(\"k neighbors\")\n",
    "plt.ylabel(\"Testing accuracy Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose the best k from above and re-fit the KNN Classifier using that k value.\n",
    "# print the score for the test data\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For this activity, `K=13` seems to be the best combination of both the train and test scores.\n",
    "\n",
    "    ![knn-train-test.png](Images/knn-train-test.png)\n",
    "\n",
    "* Ask students for any additional questions before moving on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructor Turn Activity 7 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Instructor Do: SVM (0:10)\n",
    "\n",
    "  * The goal of a linear classifier is to find a line that separates two groups of classes. However, there may be many options for choosing this line and each boundary could result in misclassification of new data.\n",
    "\n",
    "    ![linear-discriminative-classifiers.png](Images/linear-discriminative-classifiers.png)\n",
    "\n",
    "    ![classifier-boundaries.png](Images/classifier-boundaries.png)\n",
    "\n",
    "  * SVM try to find a hyperplane that maximizes the boundaries between groups. This is like building a virtual wall between groups where you want the wall to be as thick as possible.\n",
    "\n",
    "    ![svm-hyperplane.png](Images/svm-hyperplane.png)\n",
    "\n",
    "\n",
    "  * There are different kernels available for the SVM model in SciKit-Learn, but we are going to use the linear model in this example.\n",
    "\n",
    "    ![svm-linear.png](Images/svm-linear.png)\n",
    "\n",
    "  * The decision boundaries for the trained model. The algorithm shows how it maximized the boundaries between the two groups.\n",
    "\n",
    "    ![svm-boundary-plot.png](Images/svm-boundary-plot.png)\n",
    "\n",
    "  * Next, show an example of \"real\" data where the boundaries are overlapping. In this case, the svm algorithm will \"soften\" the margins and allow some of the data points to cross over the margin boundaries in order to obtain a fit.\n",
    "\n",
    "    ![svm-soften.png](Images/svm-soften.png)\n",
    "\n",
    "  * Generate a classification report to quantify and validate the model performance.\n",
    "\n",
    "    ![svm-report.png](Images/svm-report.png)\n",
    "\n",
    "  *  precision and recall to deep dive into the meaning behind each score.\n",
    "\n",
    "![svm1.png](Images/SVM1.png)\n",
    "![svm2.png](Images/SVM2.png)\n",
    "![svm3.png](Images/SVM3.png)\n",
    "![svm4.png](Images/SVM4.png)\n",
    "![svm5.png](Images/SVM5.png)\n",
    "![svm6.png](Images/SVM6.png)\n",
    "![svm7.png](Images/SVM7.png)\n",
    "![svm8.png](Images/SVM8.png)\n",
    "![svm9.png](Images/SVM9.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import style\n",
    "style.use(\"ggplot\")\n",
    "# from matplotlib import rcParams\n",
    "# rcParams['figure.figsize'] = 10, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X, y = make_blobs(n_samples=40, centers=2, random_state=42, cluster_std=1.25)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=100, cmap=\"bwr\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support vector machine linear classifier\n",
    "from sklearn.svm import SVC \n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING! BOILERPLATE CODE HERE!\n",
    "# Plot the decision boundaries\n",
    "x_min = X[:, 0].min()\n",
    "x_max = X[:, 0].max()\n",
    "y_min = X[:, 1].min()\n",
    "y_max = X[:, 1].max()\n",
    "\n",
    "XX, YY = np.mgrid[x_min:x_max, y_min:y_max]\n",
    "Z = model.decision_function(np.c_[XX.ravel(), YY.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(XX.shape)\n",
    "# plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n",
    "plt.contour(XX, YY, Z, colors=['k', 'k', 'k'],\n",
    "            linestyles=['--', '-', '--'], levels=[-.5, 0, .5])\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolor='k', s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=.95)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=100, cmap=\"bwr\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit to the training data and validate with the test data\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundaries\n",
    "x_min = X[:, 0].min()\n",
    "x_max = X[:, 0].max()\n",
    "y_min = X[:, 1].min()\n",
    "y_max = X[:, 1].max()\n",
    "\n",
    "XX, YY = np.mgrid[x_min:x_max, y_min:y_max]\n",
    "Z = model.decision_function(np.c_[XX.ravel(), YY.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(XX.shape)\n",
    "# plt.pcolormesh(XX, YY, Z > 0, cmap=plt.cm.Paired)\n",
    "plt.contour(XX, YY, Z, colors=['k', 'k', 'k'],\n",
    "            linestyles=['--', '-', '--'], levels=[-.5, 0, .5])\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolor='k', s=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions,\n",
    "                            target_names=[\"blue\", \"red\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Students Turn Activity 8 SVM\n",
    "\n",
    "* In this activity, apply a support vector machine classifier predict diabetes for the Pima Diabetes DataSet.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Import a support vector machine linear classifier and fit the model to the data.\n",
    "\n",
    "* Compute the classification report for this model using the test data.\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(\"Resources\", \"diabetes.csv\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = df[\"Outcome\"]\n",
    "target_names = [\"negative\", \"positive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop(\"Outcome\", axis=1)\n",
    "feature_names = data.columns\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a support vector machine linear classifer and fit it to the training data\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the model score using the test data\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the classification report\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * The F1 scores indicate that this model is slightly more accurate at predicting negative cases of diabetes than positive cases.\n",
    "\n",
    "    ![svm-f1.png](Images/svm-f1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructor Turn Activity 9 GridSearch\n",
    "* The code for hyperparameter tuning with `GridSearchCV`.\n",
    "\n",
    "  * The SVM model to highlight the different features available for the model. Each of these features can be adjusted and tweaked to improve model performance.\n",
    "\n",
    "    ![svm-model.png](Images/svm-model.png)\n",
    "\n",
    "  * In machine learning, there are few if any general rules on how to adjust these parameters. Instead, machine learning practitioners often use a brute force approach where they try different combinations of values to see which has the best performance. This is known as `hyperparameter tuning`\n",
    "\n",
    "  * To simplify the hyperparameter tuning process, SciKit-Learn provides a tool called `GridSearchCV`. This class is known as a `meta-estimator`. That is, it takes a model and a dictionary of parameter settings and tests all combinations of parameter settings to see which settings have the best performance.\n",
    "\n",
    "    ![grid-model.png](Images/grid-model.png)\n",
    "\n",
    "    ![grid-fit.png](Images/grid-fit.png)\n",
    "    \n",
    "    ![C1](Images/C2.png)\n",
    "    ![C1](Images/C1.png)\n",
    "    ![C1](Images/C3.png)\n",
    "\n",
    "  * Once the model has been trained, the best parameters can be accessed through the `best_params_` attribute.\n",
    "\n",
    "    ![grid-best-params.png](Images/grid-best-params.png)\n",
    "\n",
    "  * Similarly, the best score can be accessed through the `best_score_` attribute.\n",
    "\n",
    "    ![grid-best-score.png](Images/grid-best-score.png)\n",
    "\n",
    "  * The grid meta-estimator basically wraps the original model, so you can access the model functions like `predict`.\n",
    "\n",
    "    ![grid-predict.png](Images/grid-predict.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import style\n",
    "style.use(\"ggplot\")\n",
    "# from matplotlib import rcParams\n",
    "# rcParams['figure.figsize'] = 10, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "X, y = make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=.95)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=100, cmap=\"bwr\");\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the SVC Model\n",
    "from sklearn.svm import SVC \n",
    "model = SVC(kernel='linear')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the GridSearch estimator along with a parameter object containing the values to adjust\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': [1, 5, 10, 50],\n",
    "              'gamma': [0.0001, 0.0005, 0.001, 0.005]}\n",
    "grid = GridSearchCV(model, param_grid, verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using the grid search estimator. \n",
    "# This will take the SVC model and try each combination of parameters\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the best parameters for this dataset\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the best score\n",
    "print(grid)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions with the hypertuned model\n",
    "predictions = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions,\n",
    "                            target_names=[\"blue\", \"red\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Turn Activity 10 Grid Search and Hyper-Parameter Tuning\n",
    "\n",
    "* In this activity, you will revisit the SVM activity for the Pima Diabetes DataSet and apply `GridSearchCV` to tune the model parameters.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Use the starter code provided and apply `GridSearchCV` to the model. Change the `C` and `gamma` parameters.\n",
    "\n",
    "* For `C`, use the following list of settings: `[1, 5, 10]`.\n",
    "\n",
    "* For `gamma`, use the following list of settings: `[0.0001, 0.001, 0.01]`.\n",
    "\n",
    "* Print the best parameters and best score for the tuned model.\n",
    "\n",
    "* Calculate predictions using the `X_test` data and print the classification report.\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(\"Resources\", \"diabetes.csv\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = df[\"Outcome\"]\n",
    "target_names = [\"negative\", \"positive\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = df.drop(\"Outcome\", axis=1)\n",
    "feature_names = data.columns\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Support vector machine linear classifier\n",
    "from sklearn.svm import SVC \n",
    "model = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the GridSearch estimator along with a parameter object containing the values to adjust\n",
    "# Try adjusting `C` with values of 1, 5, and 10. Adjust `gamma` using .0001, 0.001, and 0.01\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit the model using the grid search estimator. \n",
    "# This will take the SVC model and try each combination of parameters\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List the best parameters for this dataset\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List the best score\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make predictions with the hypertuned model\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate classification report\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Grid Search tested our model with 27 different combinations of parameters and data. \n",
    "* Applying GridSearch saves us considerable time vs manually changing these values ourselves.\n",
    "\n",
    "  *  Knowing which parameters to tune and which values to use comes from both experience and Sklearn's documentation for their algorithms.\n",
    "\n",
    "  * By simply tuning two of our hyperparameters, the model score increased from 0.729 to an accuracy score of 0.774!\n",
    "\n",
    "    ![grid-score-diabetes.png](Images/grid-score-diabetes.png)\n",
    "\n",
    "- - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
