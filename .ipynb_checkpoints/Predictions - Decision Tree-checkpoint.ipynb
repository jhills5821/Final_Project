{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "high low span           float64\n",
       "volume change           float64\n",
       "open less than close      int64\n",
       "to buy                    int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#import data\n",
    "price_data = pd.read_csv('Model Generators/Input Data/Day Data/new data.csv')\n",
    "price_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into x and y \n",
    "X = price_data.drop(\"to buy\", axis=1)\n",
    "y = price_data[\"to buy\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "# Create a StandardScater model and fit it to the training data\n",
    "X_scaler = pickle.load(open('Model Generators/Output Models/decision model scaler.pkl', 'rb'))\n",
    "\n",
    "X_scaled = X_scaler.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy Score: 0.795\n"
     ]
    }
   ],
   "source": [
    "# Use our model to make predictions and generate confusion matrix\n",
    "dtc = pickle.load(open('Model Generators/Output Models/decision model.pkl', 'rb'))\n",
    "\n",
    "y_predict = dtc.predict(X_scaled)\n",
    "training_accur = dtc.score(X_scaled, y)\n",
    "\n",
    "print(f\"Training Accuracy Score: {training_accur}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.4958078956992524, 'high low span'),\n",
       " (0.416704971802865, 'open less than close'),\n",
       " (0.08748713249788256, 'volume change')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances = dtc.feature_importances_\n",
    "feature_names = ['high low span','volume change','open less than close']\n",
    "\n",
    "sorted(zip(dtc.feature_importances_, feature_names), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[140  36]\n",
      " [  5  19]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.80      0.87       176\n",
      "           1       0.35      0.79      0.48        24\n",
      "\n",
      "   micro avg       0.80      0.80      0.80       200\n",
      "   macro avg       0.66      0.79      0.68       200\n",
      "weighted avg       0.89      0.80      0.83       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "test_cm = confusion_matrix(y, y_predict)\n",
    "\n",
    "print(test_cm)\n",
    "\n",
    "\n",
    "test_cr = classification_report(y, y_predict)\n",
    "\n",
    "print(test_cr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.8133333333333334\n",
      "Testing Mean Squared Error (MSE): 0.18666666666666668\n"
     ]
    }
   ],
   "source": [
    "#Summary of model performance\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "testing_score = accuracy_score(y_test, y_test_predict) \n",
    "\n",
    "print(f\"Testing Accuracy: {testing_score}\")\n",
    "\n",
    "testing_mse = mean_squared_error(y_test, y_test_predict)\n",
    "\n",
    "print(f\"Testing Mean Squared Error (MSE): {testing_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_data = pd.read_csv('Model Generators/Input Data/Day Data/new data complete.csv')\n",
    "\n",
    "price_data[\"Prediction\"]=y_predict\n",
    "price_data.to_csv('Output Data/predicted_new_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
